{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thejayden/FYP/blob/main/dcgan_for_cifar_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAgBow1y2b5A"
      },
      "source": [
        "In this notebook, we will implement Deep Convolutional GAN (DCGAN) from [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/pdf/1511.06434.pdf). Both the generator and discriminator are made up of convolutional layers. The dataset used here is CIFAR 10 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_YmAA1-TLto"
      },
      "source": [
        "**Before executing the cell, go to Runtime -> Change Runtime Type -> GPU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hWFkFHjnIe_D"
      },
      "outputs": [],
      "source": [
        "import time \n",
        "\n",
        "# for building GAN\n",
        "import torch\n",
        "import torch.nn as nn  \n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "# for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from torchvision.utils import make_grid\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPVurLOvkLai"
      },
      "source": [
        "# Using GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWk_20lAknAx",
        "outputId": "ab05d7b7-a4f6-49cb-88e5-2f5abde156a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcNkwbSSkzQ5",
        "outputId": "4aed3896-9dae-4333-9fcd-7c90191d1638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "X = torch.randn(3, 2).to(device)\n",
        "print(X.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqYWbrjglYou"
      },
      "source": [
        "# Prepare Cifar10 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyxfoXZOGdzE"
      },
      "source": [
        "### Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "0edc2850fbca4b6eb3a026f2e356fdb0",
            "9910e676a0894f61acd62b9802f1e85c",
            "61c4818b2de3419cb62181e08701b222",
            "64b1ebd397a6470c9a65c30b441368da",
            "84d558778d8949a88095392c7821de0f",
            "6d68408abe7d4e6e951bda13c959a729",
            "ab0a1dc431da41b1be271bcf485839f2",
            "92721406abc3462788cb1af12d009de0",
            "67766b99cf9d48e19170dd5e01d0c710",
            "b0a714ddd79d46229acabf30e44a5bd5",
            "a97a188bc49040738844df2555a30453"
          ]
        },
        "id": "m_xzRwUypzdf",
        "outputId": "e83371bd-b301-47e5-c94a-5a55af2a5d53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to input/data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0edc2850fbca4b6eb3a026f2e356fdb0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting input/data/cifar-10-python.tar.gz to input/data\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), \n",
        "    (0.5, 0.5, 0.5)),\n",
        "])\n",
        "dataset = dset.CIFAR10(\n",
        "    root='input/data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "batch_size = 128        # Batch size during training\n",
        "\n",
        "# Create the dataloader\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_NABQdZqNbJ",
        "outputId": "5ffa5f86-d32a-4184-de51-1335edd2fb81"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8Lxk3m-HD2t"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0AxuKIrohnx5"
      },
      "outputs": [],
      "source": [
        "label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWah1kVBuV9s",
        "outputId": "f5a4626f-6aac-456b-d96a-ec668689acd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[[ 0.5137,  0.5843,  0.5608,  ..., -0.3176, -0.1843, -0.3961],\n",
            "         [ 0.9451,  0.9843,  0.9686,  ..., -0.1765, -0.0039, -0.1529],\n",
            "         [ 0.9529,  0.9843,  0.9451,  ..., -0.1137, -0.1216, -0.1608],\n",
            "         ...,\n",
            "         [-0.0667, -0.0431, -0.0431,  ..., -0.2078, -0.1765, -0.1451],\n",
            "         [-0.0745, -0.0196, -0.0431,  ..., -0.1608, -0.1843, -0.1922],\n",
            "         [-0.2627, -0.1608, -0.1922,  ..., -0.2392, -0.2627, -0.2863]],\n",
            "\n",
            "        [[ 0.5373,  0.5608,  0.5451,  ..., -0.3255, -0.2392, -0.4510],\n",
            "         [ 0.9686,  0.9686,  0.9686,  ..., -0.1765, -0.0275, -0.1765],\n",
            "         [ 0.9608,  0.9686,  0.9608,  ..., -0.1294, -0.1373, -0.1608],\n",
            "         ...,\n",
            "         [ 0.0039,  0.0431,  0.0667,  ..., -0.1216, -0.1216, -0.1137],\n",
            "         [-0.0118,  0.0431,  0.0510,  ..., -0.0902, -0.1294, -0.1529],\n",
            "         [-0.2235, -0.1137, -0.1294,  ..., -0.2078, -0.2157, -0.2392]],\n",
            "\n",
            "        [[ 0.6000,  0.6314,  0.6157,  ..., -0.5686, -0.3804, -0.6078],\n",
            "         [ 0.9765,  0.9922,  0.9843,  ..., -0.6235, -0.4118, -0.5608],\n",
            "         [ 0.9373,  0.9529,  0.9373,  ..., -0.6392, -0.6078, -0.6314],\n",
            "         ...,\n",
            "         [ 0.0824,  0.1686,  0.2000,  ..., -0.0510, -0.0510, -0.0353],\n",
            "         [ 0.0824,  0.2000,  0.2078,  ...,  0.0353,  0.0118, -0.0118],\n",
            "         [-0.0667,  0.0980,  0.0902,  ..., -0.0196, -0.0039, -0.0431]]]), 9)\n"
          ]
        }
      ],
      "source": [
        "# Show a random image\n",
        "idx = np.random.choice(len(dataset)) # get a random index in range [0, len(dataset))\n",
        "img, label = dataset[idx]\n",
        "\n",
        "print(dataset[idx])\n",
        "print('Image size: {}'.format(img.shape))\n",
        "print('Label: {}'.format(label_names[label]))\n",
        "plt.axis('off')\n",
        "# torch.squeeze() method removes the dimensions of input of size 1\n",
        "# input size of 32 x 32 x 1 will become 32 x 32\n",
        "plt.imshow(img.permute(1, 2, 0).squeeze(), cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVVKyIglP0Tx"
      },
      "outputs": [],
      "source": [
        "def show_images(image_tensor, num_images=25, nrow=5, save=False): \n",
        "  image_tensor = image_tensor.detach().to('cpu') \n",
        "  # convert the output values to the expected range of float pixel values [0, 1]\n",
        "  image_tensor = (image_tensor + 1)/2  \n",
        "  img = make_grid(image_tensor[:num_images], nrow=nrow).permute(1,2,0).squeeze()\n",
        "  # if save is True, just return the image  \n",
        "  if save:\n",
        "    return img\n",
        "  plt.axis('off')\n",
        "  plt.imshow(img)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OXovjSXk-Rd"
      },
      "outputs": [],
      "source": [
        "for i, data in enumerate(dataloader):\n",
        "  X, _ = data  \n",
        "  print(X.shape)\n",
        "  break\n",
        "\n",
        "show_images(X, 64, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBYvp0MuIrbZ"
      },
      "source": [
        "# Model definition\n",
        "In this section, we will define generator and discriminator structure.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVcpMWROe9Ad"
      },
      "source": [
        "### Generator Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWQn7lEgpF6c"
      },
      "source": [
        "**Each layer of generator:**\n",
        "\n",
        "\n",
        "*   Transposed convolution for upsampling\n",
        "*   Use batchnorm except for the last layer\n",
        "*   Apply ReLU activation for all layers except for the output, which uses Tanh\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENQUy5hkKD6-"
      },
      "source": [
        "\n",
        "Remember that the output size of transposed convolution is:\n",
        "$$output size = (input size -1)*stride - 2*padding + kernel size$$\n",
        "\n",
        "Useful functions in building generator:\n",
        "\n",
        "\n",
        "*   [ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html)\n",
        "*   [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html?highlight=batchnorm#torch.nn.BatchNorm2d)\n",
        "*   [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html?highlight=relu#torch.nn.ReLU)\n",
        "*   [Tanh](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html?highlight=tanh#torch.nn.Tanh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEYGqBE_ml0U"
      },
      "outputs": [],
      "source": [
        "nz = 100                # Size of z latent vector (i.e. size of generator input)\n",
        "nc = 3                  # Number of channels in the training images. For color images this is 3\n",
        "hidden_dim = 64         # Size of feature maps in generator and discriminator for CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxkG7ZWkOEDb"
      },
      "outputs": [],
      "source": [
        "# Generator Code\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # layer 1, input is Z (noise), going into a convolution\n",
        "            nn.ConvTranspose2d(in_channels=nz, out_channels=hidden_dim * 8, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(hidden_dim * 8),\n",
        "            nn.ReLU(True),\n",
        "            # size (hidden_dim*8) x 4 x 4\n",
        "\n",
        "            # layer 2\n",
        "            nn.ConvTranspose2d(in_channels=hidden_dim * 8, out_channels=hidden_dim * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(hidden_dim * 4),\n",
        "            nn.ReLU(True),\n",
        "            # size (hidden_dim*2) x 8 x 8\n",
        "\n",
        "            # layer 3\n",
        "            nn.ConvTranspose2d(in_channels=hidden_dim * 4, out_channels=hidden_dim * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(hidden_dim * 2),\n",
        "            nn.ReLU(True),\n",
        "            # size (hidden_dim) x 16 x 16\n",
        "\n",
        "            # layer 4 (last layer)\n",
        "            # state size. (hidden_dim*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(in_channels=hidden_dim * 2, out_channels=nc, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "         # state size. (nc) x 32 x 32\n",
        "\n",
        "    def forward(self, input):\n",
        "        '''\n",
        "        Function for completing a forward pass of the classifier: Given an image tensor, \n",
        "        returns an n_classes-dimension tensor representing fake/real.\n",
        "        Parameters:\n",
        "            image: a flattened image tensor with im_chan channels\n",
        "        '''\n",
        "        return self.main(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDK6COU9SE2D"
      },
      "outputs": [],
      "source": [
        "# apply to Generator and Discriminator network\n",
        "def init_weights(m):\n",
        "  if type(m) == nn.Conv2d or type(m) == nn.ConvTranspose2d:\n",
        "    nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "  elif type(m) == nn.BatchNorm2d:\n",
        "    nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "    nn.init.constant_(m.bias, val=0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlnjhzDrfBWk"
      },
      "outputs": [],
      "source": [
        "# Create the generator\n",
        "netG = Generator().to(device)\n",
        "\n",
        "# Apply the init_weights function to randomly initialize all weights\n",
        "#  to mean=0, stdev=0.02.\n",
        "netG.apply(init_weights)\n",
        "\n",
        "# Print the model\n",
        "print(netG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iFRpp48ojgo"
      },
      "outputs": [],
      "source": [
        "def noise_vector(num, dim):\n",
        "  '''\n",
        "  Function for creating noise vectors: Given the dimensions (num, dim)\n",
        "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
        "    \n",
        "  num: number of noise vectors, num = batch size in training process\n",
        "  dim: dimension of each noise vector\n",
        "\n",
        "  return: noise vectors of shape (num, dim, 1, 1)\n",
        "  '''\n",
        "  return torch.randn(num, dim, 1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roIkeHqso5XA"
      },
      "outputs": [],
      "source": [
        "z = noise_vector(1, 100)\n",
        "print(z.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-cEfpmqsdnd"
      },
      "outputs": [],
      "source": [
        "z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9wEKG3OPBA7"
      },
      "outputs": [],
      "source": [
        "gen = Generator()\n",
        "fake = gen(z).detach()\n",
        "print(fake[0].shape)\n",
        "\n",
        "plt.axis('off')\n",
        "plt.imshow(fake[0].permute(1, 2, 0).squeeze(), cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBsDIxZvfGC1"
      },
      "source": [
        "### Discriminator Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7ab8yAIUZ5D"
      },
      "source": [
        "**Each layer of discriminator:**\n",
        "\n",
        "\n",
        "*   Convolution for downsampling\n",
        "*   Use batchnorm except for the last layer\n",
        "*   Apply LeakyReLU activation with slope of 0.2 for all layers \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6GgdsgrKa7e"
      },
      "source": [
        "Remember that the output size of convolution is:\n",
        "$$output size = (inputsize + 2*padding - kernelsize)/stride + 1$$\n",
        "\n",
        "Useful functions in building discriminator:\n",
        "\n",
        "\n",
        "*   [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv2d#torch.nn.Conv2d)\n",
        "*   [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html?highlight=batchnorm#torch.nn.BatchNorm2d)\n",
        "*   [LeakyReLU](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html?highlight=leaky%20relu#torch.nn.LeakyReLU)\n",
        "*   [Sigmoid](https://pytorch.org/docs/stable/nn.functional.html?highlight=sigmoid#torch.nn.functional.sigmoid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcs3aV9WVwTp"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # layer 1, input is image of size nc x 32 x 32\n",
        "            nn.Conv2d(nc, hidden_dim, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (hidden_dim) x 16 x 16\n",
        "\n",
        "            # layer 2\n",
        "            nn.Conv2d(hidden_dim, hidden_dim * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(hidden_dim * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (hidden_dim*2) x 8 x 8\n",
        "\n",
        "            # layer 3\n",
        "            nn.Conv2d(hidden_dim * 2, hidden_dim * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(hidden_dim * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "              # state size. (hidden_dim*4) x 4 x 4\n",
        "\n",
        "            # layer 4\n",
        "            nn.Conv2d(hidden_dim * 4, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "            # output size 1 (probability)\n",
        "        )\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSM7oQjRbvHb"
      },
      "outputs": [],
      "source": [
        "# Create the Discriminator\n",
        "netD = Discriminator().to(device)\n",
        "\n",
        "# Apply the init_weights function to randomly initialize all weights\n",
        "#  to mean=0, stdev=0.2.\n",
        "netD.apply(init_weights)\n",
        "\n",
        "# Print the model\n",
        "print(netD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvVfno2cfI6P"
      },
      "source": [
        "# Start Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFf8VpsL3x8M"
      },
      "source": [
        "Useful function:\n",
        "\n",
        "\n",
        "\n",
        "*   [torch.optim.Adam](https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U-JOcK8XuhU"
      },
      "outputs": [],
      "source": [
        "lr = 0.0002                         # Learning rate for optimizers\n",
        "beta1 = 0.5                         # Beta1 hyperparam for Adam optimizers\n",
        "\n",
        "criterion = nn.BCELoss()            # Loss function\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "# Latent vectors to visualize the progression of the generator\n",
        "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1 \n",
        "fake_label = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7hXgCW0iyI8"
      },
      "outputs": [],
      "source": [
        "num_epochs = 30          # Number of training epochs\n",
        "\n",
        "# Lists to keep track of progress\n",
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "\n",
        "        ############################\n",
        "        # (1) Update D network\n",
        "        ###########################\n",
        "\n",
        "        netD.zero_grad()\n",
        "\n",
        "        ## Train with all-real batch\n",
        "        # Format batch\n",
        "        real_cpu = data[0].to(device)\n",
        "        b_size = real_cpu.size(0)\n",
        "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "        \n",
        "        # Forward pass real batch through D\n",
        "        output = netD(real_cpu).view(-1)\n",
        "\n",
        "        # Calculate loss on all-real batch\n",
        "        errD_real = criterion(output, label)\n",
        "\n",
        "        # Calculate gradients for D in backward pass\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        ## Train with all-fake batch\n",
        "        # Generate batch of latent vectors\n",
        "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "\n",
        "        # Generate fake image batch with G\n",
        "        fake = netG(noise)\n",
        "        label.fill_(fake_label)\n",
        "\n",
        "        # Classify all fake batch with D\n",
        "        output = netD(fake.detach()).view(-1)\n",
        "\n",
        "        # Calculate D's loss on the all-fake batch\n",
        "        errD_fake = criterion(output, label)\n",
        "\n",
        "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "\n",
        "        # Compute error of D as sum over the fake and the real batches\n",
        "        errD = errD_real + errD_fake\n",
        "        # Update D\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)        # fake labels are real for generator cost\n",
        "\n",
        "        # Classify all fake batch with D \n",
        "        output = netD(fake).view(-1)\n",
        "\n",
        "        # Calculate G's loss based on this output\n",
        "        errG = criterion(output, label)\n",
        "\n",
        "        # Calculate gradients for G\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        \n",
        "        # Update G\n",
        "        optimizerG.step()\n",
        "        \n",
        "        # Output training stats\n",
        "        if i % 50 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, num_epochs, i, len(dataloader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "        \n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "        \n",
        "        # Check how the generator is doing by saving G's output on fixed_noise\n",
        "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
        "            with torch.no_grad():\n",
        "                fake = netG(fixed_noise).detach().cpu()\n",
        "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "            \n",
        "        iters += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4xmEPpRdKXV"
      },
      "outputs": [],
      "source": [
        "torch.save(netG.state_dict(), 'generator.pth')\n",
        "torch.save(netD.state_dict(), 'discriminator.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BPp8cIIfQbI"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgHNW8IHfR6d"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18,5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.xscale('linear')\n",
        "plt.ylim(0, 15)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsuDe-c503DK"
      },
      "outputs": [],
      "source": [
        "#%%capture\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
        "\n",
        "HTML(ani.to_jshtml())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model\n",
        "# Remember to define hyper parameters such as nz, nc before loading\n",
        "\n",
        "# Uncomment this part\n",
        "# netG = Generator().to(device)\n",
        "# netG.load_state_dict(torch.load(\"./generator.pth\"))\n",
        "# netG.eval()\n",
        "\n",
        "# netD = Discriminator().to(device)\n",
        "# netD.load_state_dict(torch.load(\"./discriminator.pth\"))\n",
        "# netD.eval()\n",
        "# print(\"Loaded the models!\")\n",
        "\n",
        "# opt = torch.optim.Adam(netD.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "jpTaogDr_lgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0B14k-ie7B69"
      },
      "outputs": [],
      "source": [
        "# noise vector\n",
        "noise = noise_vector(25, nz).to(device)\n",
        "# pass the noise vectors to trained generator\n",
        "imgs = netG(noise)\n",
        "show_images(imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "285cmBrQ2mCZ"
      },
      "source": [
        "# What's next\n",
        "\n",
        "\n",
        "*   Using other datasets to train a GAN, for example [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) which is also available in *torchvision*\n",
        "*   Explore methods to improve the stability of GAN learning. For anyone who is interested in this, can read more on Wasserstein GAN and Gradient Penalty. [WGAN-GP](https://jonathan-hui.medium.com/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490)\n",
        "*   Explore Conditional GAN, which allows you to control the output.\n",
        "\n",
        "Want to include GAN in your next project? Take a look at this Github repository [GANs Awesome Applications](https://github.com/nashory/gans-awesome-applications)!\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "dcgan_for_cifar_solution.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0edc2850fbca4b6eb3a026f2e356fdb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9910e676a0894f61acd62b9802f1e85c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_61c4818b2de3419cb62181e08701b222",
              "IPY_MODEL_64b1ebd397a6470c9a65c30b441368da",
              "IPY_MODEL_84d558778d8949a88095392c7821de0f"
            ]
          }
        },
        "9910e676a0894f61acd62b9802f1e85c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "61c4818b2de3419cb62181e08701b222": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6d68408abe7d4e6e951bda13c959a729",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab0a1dc431da41b1be271bcf485839f2"
          }
        },
        "64b1ebd397a6470c9a65c30b441368da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_92721406abc3462788cb1af12d009de0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_67766b99cf9d48e19170dd5e01d0c710"
          }
        },
        "84d558778d8949a88095392c7821de0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b0a714ddd79d46229acabf30e44a5bd5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:10&lt;00:00, 17678572.32it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a97a188bc49040738844df2555a30453"
          }
        },
        "6d68408abe7d4e6e951bda13c959a729": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab0a1dc431da41b1be271bcf485839f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "92721406abc3462788cb1af12d009de0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "67766b99cf9d48e19170dd5e01d0c710": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b0a714ddd79d46229acabf30e44a5bd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a97a188bc49040738844df2555a30453": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}